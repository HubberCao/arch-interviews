
一、如何保障数据不丢失

（一）RabbitMQ之如何保障数据不丢失
	1、消费者实例宕机的时候，如何保障数据是不会丢失？

	手动ack机制非常的简单，必须要消费者确保自己处理完毕了一个消息，才能手动发送ack给MQ，MQ收到ack之后才会删除这个消息，如果消费者还没发送ack，消费者自己就宕机了，此时MQ感知到它的宕机，就会重新投递这条消息给其他的消费者实例。
	通过这种机制保证消费者实例宕机的时候，数据是不会丢失的。

	如果采用手动ack机制，实际上消费者服务每次消费了一条消息，处理完毕完成消费之后，就会发送一个ack消息给RabbitMQ服务器，这个ack消息是会带上自己本次消息的delivery tag的。
	这里大家必须注意的一点，就是delivery tag仅仅在一个channel内部是唯一标识消息投递的。所以说，你ack一条消息的时候，必须是通过接受这条消息的同一个channel来进行。

	channel.basicAck(
	            delivery.getEnvelope().getDeliveryTag(), 
	            false);//表给示消费者成功消费，返回给MQ
	            
	2、分析手动ack和默认自动ack区别？
	①实际上默认用自动ack，是非常简单的。RabbitMQ只要投递一个消息出去给仓储服务，那么他立马就把这个消息给标记为删除，因为他是不管消费者服务到底接收到没有，或者处理完没有。所以这种情况下，性能很好，但是数据容易丢失。

	②如果手动ack，那么就是必须等消费者服务完成消费以后，才会手动发送ack给RabbitMQ，此时RabbitMQ才会认为消息处理完毕，然后才会标记消息为删除。这样在发送ack之前，消费者服务宕机，RabbitMQ会重发消息给另外一个消费者服务实例，保证数据不丢。

	3、RabbitMQ弄丢了数据？

	就是RabbitMQ自己弄丢了数据，这个你必须开启RabbitMQ的持久化，就是消息写入之后会持久化到磁盘，哪怕是RabbitMQ自己挂了，恢复之后会自动读取之前存储的数据，一般数据不会丢。
	除非极其罕见的是，RabbitMQ还没持久化，自己就挂了，可能导致少量数据会丢失的，但是这个概率较小。

	设置持久化有两个步骤，第一个是创建queue的时候将其设置为持久化的，这样就可以保证RabbitMQ持久化queue的元数据，但是不会持久化queue里的数据；
	第二个是发送消息的时候将消息的deliveryMode设置为2，就是将消息设置为持久化的，此时RabbitMQ就会将消息持久化到磁盘上去。必须要同时设置这两个持久化才行，RabbitMQ哪怕是挂了，再次重启，也会从磁盘上重启恢复queue，恢复这个queue里的数据。

	而且持久化可以跟生产者那边的confirm机制配合起来，只有消息被持久化到磁盘之后，才会通知生产者ack了，所以哪怕是在持久化到磁盘之前，RabbitMQ挂了，数据丢了，生产者收不到ack，你也是可以自己重发的。

	哪怕是你给RabbitMQ开启了持久化机制，也有一种可能，就是这个消息写到了RabbitMQ中，但是还没来得及持久化到磁盘上，结果不巧，此时RabbitMQ挂了，就会导致内存里的一点点数据会丢失。
	
	4、如何保证生产者投递到消息中间件(MQ)的消息不丢失？
	问题：如果投递出去的消息在网络传输过程中丢失，或者在RabbitMQ的内存中还没写入磁盘的时候宕机，都会导致生产端投递到MQ的数据丢失。而且丢失之后，生产者自己还感知不到，同时还没办法来补救。

	生产者需要开启confirm模式，投递消息到MQ，如果MQ一旦将消息持久化到磁盘之后，必须也要回传一个confirm消息给生产端。这样的话，如果生产端的服务接收到了这个confirm消息，就知道是已经持久化到磁盘了。
	如果没有接收到confirm消息，那么就说明这条消息半路可能丢失了，此时你就可以重新投递消息到MQ去，确保消息不要丢失。

	而且一旦你开启了confirm模式之后，每次消息投递也同样是有一个delivery tag的，也是起到唯一标识一次消息投递的作用。这样，MQ回传ack给生产端的时候，会带上这个delivery tag。你就知道具体对应着哪一次消息投递了，可以删除这条消息。

	此外，如果RabbitMQ接收到一条消息之后，结果内部出错发现无法处理这条消息，那么MQ会回传一个nack消息给生产者。此时生产者就会感知到这条消息可能处理有问题，你可以选择重新再次投递这条消息到MQ去。

	或者另一种情况，如果某条消息很长时间都没给你回传ack/nack，那可能是极端意外情况发生了，数据也丢了，你也可以自己重新投递消息到MQ去。
	
	事务机制和cnofirm机制最大的不同在于，事务机制是同步的，你提交一个事务之后会阻塞在那儿，但是confirm机制是异步的，你发送个消息之后就可以发送下一个消息，然后那个消息RabbitMQ接收了之后会异步回调你一个接口通知你这个消息接收到了。
	所以一般在生产者这块避免数据丢失，都是用confirm机制的。

	5、confirm机制投递消息的高延迟性
	
	一旦启用了confirm机制投递消息到MQ之后，MQ是不保证什么时候会给你一个ack或者nack的。

	因为RabbitMQ自己内部将消息持久化到磁盘，本身就是通过异步批量的方式来进行的。正常情况下，你投递到RabbitMQ的消息都会先驻留在内存里，然后过了几百毫秒的延迟时间之后，再一次性批量把多条消息持久化到磁盘里去。
	这样做，是为了兼顾高并发写入的吞吐量和性能的，因为要是你来一条消息就写一次磁盘，那么性能会很差，每次写磁盘都是一次fsync强制刷入磁盘的操作，是很耗时的。

	那如何解决呢？

	绝对不能以同步写消息 + 等待ack的方式来投递消息，用来临时存放未ack消息的存储需要承载高并发写入，而且我们不需要什么复杂的运算操作，这种存储首选绝对不是MySQL之类的关系数据库，而建议采用kv存储。
	kv存储承载高并发能力极强，而且kv操作性能很高。

	生产者消息投递出去之后并且在kv存储器存储，这个投递的线程其实就可以返回了，至于每个消息的异步回调，是通过在channel注册一个confirm监听器实现的。
	生产者收到一个消息ack之后，就从kv存储中删除这条临时消息；收到一个消息nack之后，就从kv存储提取这条消息然后重新投递一次即可；也可以自己对kv存储里的消息做监控，如果超过一定时长没收到ack，就主动重发消息。
	
	(二) kafka
	
	（1）消费端弄丢了数据

	唯一可能导致消费者弄丢数据的情况，就是说，你那个消费到了这个消息，然后消费者那边自动提交了offset，让kafka以为你已经消费好了这个消息，其实你刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。

	大家都知道kafka会自动提交offset，那么只要关闭自动提交offset，在处理完之后自己手动提交offset，就可以保证数据不会丢。
	但是此时确实还是会重复消费，比如你刚处理完，还没提交offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。

	生产环境碰到的一个问题，就是说我们的kafka消费者消费到了数据之后是写到一个内存的queue里先缓冲一下，结果有的时候，你刚把消息写入内存queue，然后消费者会自动提交offset。

	然后此时我们重启了系统，就会导致内存queue里还没来得及处理的数据就丢失了

	（2）kafka弄丢了数据

	这块比较常见的一个场景，就是kafka某个broker宕机，然后重新选举partiton的leader时。大家想想，要是此时其他的follower刚好还有些数据没有同步，
	结果此时leader挂了，然后选举某个follower成leader之后，他不就少了一些数据？这就丢了一些数据啊。

	生产环境也遇到过，我们也是，之前kafka的leader机器宕机了，将follower切换为leader之后，就会发现说这个数据就丢了。

	所以此时一般是要求起码设置如下4个参数：

	给这个topic设置replication.factor参数：这个值必须大于1，要求每个partition必须有至少2个副本。
	在kafka服务端设置min.insync.replicas参数：这个值必须大于1，这个是要求一个leader至少感知到有至少一个follower还跟自己保持联系，没掉队，这样才能确保leader挂了还有一个follower吧。
	在producer端设置acks=all：这个是要求每条数据，必须是写入所有replica之后，才能认为是写成功了。
	在producer端设置retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。
	（3）生产者会不会弄丢数据

	如果按照上述的思路设置了ack=all，一定不会丢，要求是，你的leader接收到消息，所有的follower都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。
	

二、如何保证消息的顺序性
	rabbitmq：一个queue，多个consumer，这不明显乱了
	kafka：一个topic，一个partition，一个consumer，内部多线程，这不也明显乱了
　　
	如何来保证消息的顺序性呢？
	rabbitmq：拆分多个queue，每个queue一个consumer，就是多一些queue而已，确实是麻烦点；或者就一个queue但是对应一个consumer，然后这个consumer内部用内存队列做排队，然后分发给底层不同的worker来处理。
	kafka：一个topic，一个partition，一个consumer，内部单线程消费，写N个内存queue，然后N个线程分别消费一个内存queue即可。	

三、消息重复

	消费端处理消息的业务逻辑保持幂等性
	保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现	
	
四、如何解决消息队列的数据积压

	设立过期时间，直接丢弃数据
	恢复消费者，临时扩容，快速消费

	一个消费者一秒是1000条，一秒3个消费者是3000条，一分钟是18万条，1000多万条，所以如果你积压了几百万到上千万的数据，即使消费者恢复了，也需要大概1小时的时间才能恢复过来。

	一般这个时候，只能操作临时紧急扩容了，具体操作步骤和思路如下：

	先修复consumer的问题，确保其恢复消费速度，然后将现有cnosumer都停掉。
	新建一个topic，partition是原来的10倍，临时建立好原先10倍或者20倍的queue数量。
	然后写一个临时的分发数据的consumer程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的10倍数量的queue。
	接着临时征用10倍的机器来部署consumer，每一批consumer消费一个临时queue的数据。
	这种做法相当于是临时将queue资源和consumer资源扩大10倍，以正常的10倍速度来消费数据。
	等快速消费完积压数据之后，得恢复原先部署架构，重新用原先的consumer机器来消费消息。	
	
五、如果让你设计一个MQ，你怎么设计
（1）首先这个mq得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？
	设计个分布式的系统呗，参照一下kafka的设计理念，broker -> topic -> partition，每个partition放一个机器，就存一部分数据。
	如果现在资源不够了，简单啊，给topic增加partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？

（2）其次你得考虑一下这个mq的数据要不要落地磁盘吧？那肯定要了，落磁盘，才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是kafka的思路。

（3）其次你考虑一下你的mq的可用性啊？这个事儿，具体参考我们之前可用性那个环节讲解的kafka的高可用保障机制。多副本 -> leader & follower -> broker挂了重新选举leader即可对外服务。

（4）能不能支持数据0丢失啊？可以的，参考我们之前说的那个kafka数据零丢失方案

其实一个mq肯定是很复杂的，其实这是个开放题，就是看看你有没有从架构角度整体构思和设计的思维以及能力	
